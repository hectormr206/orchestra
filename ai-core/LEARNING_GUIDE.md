# GuÃ­a de Inicio RÃ¡pido - Sistema de Aprendizaje AI-Core

> **Aprende automÃ¡ticamente de la experiencia para optimizar decisiones**

---

## ğŸ¯ Â¿QuÃ© es el Sistema de Aprendizaje?

El sistema de aprendizaje de AI-Core usa **Aprendizaje por Refuerzo (Actor-Critic)** para:
- Aprender quÃ© skills y agentes usar para cada tarea
- Optimizar estrategias de ejecuciÃ³n basÃ¡ndose en experiencia real
- Mejorar continuamente con cada tarea ejecutada

**En resumen:** Entre mÃ¡s uses, mejor se vuelve.

---

## ğŸš€ Usar el Sistema de Aprendizaje

### Modo 1: Recolectar Experiencias (Sin Afectar Decisiones)

```bash
# Habilitar modo shadow (solo recolecta datos, no cambia decisiones)
export AI_CORE_LEARNING_MODE=shadow

# Usa ai-core normalmente
# Las experiencias se guardan automÃ¡ticamente en data/experience_buffer/

# Verificar recolecciÃ³n
python SKILLS/learning/assets/monitor.py --data data/experience_buffer/experiences.jsonl
```

**QuÃ© esperar:**
- âœ… AI-Core funciona **normalmente** (usa reglas)
- âœ… Experiencias se guardan automÃ¡ticamente
- âœ… Sin riesgo en producciÃ³n

### Modo 2: A/B Testing (Validar antes de usar)

```bash
# 10% usa aprendizaje, 90% usa reglas
export AI_CORE_LEARNING_MODE=ab_test

# Verificar performance
python SKILLS/learning/assets/compare_policies.py \
  --learned data/models/actor_checkpoint_v1.0.pt
```

**QuÃ© esperar:**
- âœ… Testing seguro del modelo aprendido
- âœ… ComparaciÃ³n directa con reglas
- âœ… MÃ©tricas de mejora

### Modo 3: ProducciÃ³n (Uso completo)

```bash
# Usa polÃ­tica aprendida (con fallback a reglas si hay poca confianza)
export AI_CORE_LEARNING_MODE=production

# Umbral de confianza
export AI_CORE_CONFIDENCE_THRESHOLD=0.8
```

**QuÃ© esperar:**
- âœ… Decisiones optimizadas automÃ¡ticamente
- âœ… Fallback automÃ¡tico a reglas si no estÃ¡ seguro
- âœ… Monitoreo continuo de performance

---

## ğŸ“Š CÃ³mo Funciona el Aprendizaje

### 1. Recolecta Experiencias

Cada vez que AI-Core ejecuta una tarea, guarda:

```yaml
Experiencia:
  estado:
    - Tipo de tarea (feature, bug, refactor, etc.)
    - Dominio (backend, frontend, database, etc.)
    - Complejidad (simple, medium, complex)
    - Contexto (tiempo estimado, recursos necesarios)

  acciÃ³n:
    - Skills usados
    - Agents usados
    - Estrategia de ejecuciÃ³n

  resultado:
    - Recompensa: +100 (Ã©xito) o -100 (fallo)
    - Tiempo real vs estimado
    - Errores
    - Recursos usados vs necesarios
```

### 2. Entrena el Modelo

```bash
# Entrenar con experiencias recolectadas
python SKILLS/learning/assets/train.py \
  --data data/experience_buffer/experiences.jsonl \
  --epochs 100 \
  --output-dir data/models
```

**Resultado:** Un modelo que predice la mejor acciÃ³n para cada estado

### 3. EvalÃºa y Despliega

```bash
# Evaluar modelo
python SKILLS/learning/assets/evaluate.py \
  --model data/models/actor_checkpoint_v1.0.pt

# Si es mejor que reglas, desplegar
export AI_CORE_LEARNING_MODE=production
```

---

## ğŸ§ª Probar el Sistema (Con Datos SintÃ©ticos)

Si quieres probar sin esperar a recolectar datos reales:

```bash
# 1. Generar datos de prueba
python SKILLS/learning/assets/generate_test_data.py \
  --count 1000 \
  --output data/experience_buffer/test_experiences.jsonl

# 2. Entrenar modelo
python SKILLS/learning/assets/train.py \
  --data data/experience_buffer/test_experiences.jsonl \
  --epochs 50

# 3. Evaluar
python SKILLS/learning/assets/evaluate.py \
  --model data/models/actor_checkpoint_v1.0.pt \
  --test-data data/experience_buffer/test_experiences.jsonl
```

**Resultado:** Modelo entrenado que puedes usar inmediatamente

---

## ğŸ“ˆ Monitorear el Sistema

### Ver Dashboard en Tiempo Real

```bash
# Monitoreo one-time
python SKILLS/learning/assets/monitor.py \
  --data data/experience_buffer/experiences.jsonl

# Monitoreo continuo (refresca cada 30s)
python SKILLS/learning/assets/monitor.py \
  --data data/experience_buffer/experiences.jsonl \
  --watch
```

**MÃ©tricas que muestra:**
- Total de experiencias recolectadas
- Recompensa promedio
- Success rate
- DistribuciÃ³n por tipo de tarea
- Dominios con mejor performance
- Complejidades mÃ¡s difÃ­ciles

---

## ğŸ¯ Casos de Uso Reales

### Ejemplo 1: Proyecto Nuevo

```bash
# 1. Instala ai-core
cd mi-proyecto
git clone https://github.com/hectormr206/ai-core.git ai-core
cd ai-core && ./run.sh

# 2. Habilita modo shadow para recolectar datos
export AI_CORE_LEARNING_MODE=shadow

# 3. Trabaja normalmente por 2 semanas
# Las experiencias se recolectan automÃ¡ticamente

# 4. Entrena primer modelo
python SKILLS/learning/assets/train.py \
  --data data/experience_buffer/experiences.jsonl \
  --epochs 100

# 5. EvalÃºa y si es bueno, habilita ab_test
export AI_CORE_LEARNING_MODE=ab_test

# 6. Si mejora > 15%, habilita production
export AI_CORE_LEARNING_MODE=production
```

### Ejemplo 2: Proyecto Ya Existente

```bash
# 1. Ya tienes ai-core instalado
# 2. Habilita learning
export AI_CORE_LEARNING_MODE=shadow

# 3. Recolecta datos mientras trabajas normalmente
# (2-4 semanas recomendado)

# 4. Entrena cuando tengas 1000+ experiencias
python SKILLS/learning/assets/train.py \
  --data data/experience_buffer/experiences.jsonl

# 5. Despliega gradualmente
```

---

## ğŸ“Š Entender las MÃ©tricas

### Durante Entrenamiento

```
Epoch   0: Actor Loss=242.30, Reward=70.94,  Entropy=3.40
Epoch  10: Actor Loss=257.24, Reward=84.21, Entropy=3.39
Epoch  20: Actor Loss=186.92, Reward=83.25, Entropy=3.18
Epoch  30: Actor Loss=118.16, Reward=78.82, Entropy=2.53
Epoch  49: Actor Loss=100.65, Reward=79.48, Entropy=1.88
```

**QuÃ© significa:**
- **Actor Lossâ†“**: Mejorando (debe disminuir)
- **Reward+:** Aprendiendo (debe ser positivo)
- **Entropyâ†“**: MÃ¡s confiado (disminuye lentamente)

### Durante EvaluaciÃ³n

```
Accuracy: 75.3%         â†’ Target: > 70% âœ…
Success Rate: 85.2%      â†’ Target: > 70% âœ…
Mean Reward: +45.30       â†’ Target: > 0 âœ…
Correlation: 0.42         â†’ Target: > 0.3 âœ…
```

**Si todo estÃ¡ en âœ…, el modelo estÃ¡ listo para producciÃ³n.**

---

## ğŸ› ï¸ Comandos Ãštiles

```bash
# Generar datos de prueba
python SKILLS/learning/assets/generate_test_data.py --count 1000

# Entrenar modelo
python SKILLS/learning/assets/train.py --epochs 100

# Evaluar modelo
python SKILLS/learning/assets/evaluate.py \
  --model data/models/actor_checkpoint_v1.0.pt

# Comparar con baseline
python SKILLS/learning/assets/compare_policies.py \
  --learned data/models/actor_checkpoint_v1.0.pt

# Monitorear
python SKILLS/learning/assets/monitor.py \
  --data data/experience_buffer/experiences.jsonl

# Test completo (end-to-end)
./SKILLS/learning/assets/test_pipeline.sh
```

---

## âš ï¸ Problemas Comunes

### Problema: Modelo no aprende (Reward negativo)

**Causa:** Pocas experiencias o datos de mala calidad

**SoluciÃ³n:**
```bash
# 1. Recolecta mÃ¡s datos
export AI_CORE_LEARNING_MODE=shadow
# Usa ai-core normalmente por mÃ¡s tiempo

# 2. Verifica cantidad
wc -l data/experience_buffer/experiences.jsonl
# MÃ­nimo recomendado: 1000

# 3. Verifica calidad
python SKILLS/learning/assets/monitor.py \
  --data data/experience_buffer/experiences.jsonl
```

### Problema: Performance baja despuÃ©s de desplegar

**Causa:** Modelo sobreajustado a datos de entrenamiento

**SoluciÃ³n:**
```bash
# 1. Aumenta umbral de confianza
export AI_CORE_CONFIDENCE_THRESHOLD=0.9

# 2. O vuelve a ab_test para validar
export AI_CORE_LEARNING_MODE=ab_test

# 3. Si persiste, vuelve a rules
export AI_CORE_LEARNING_MODE=disabled
```

### Problema: Quiero resetear el aprendizaje

**SoluciÃ³n:**
```bash
# 1. Deshabilita learning
export AI_CORE_LEARNING_MODE=disabled

# 2. Opcional: Borra experiencias
rm data/experience_buffer/experiences.jsonl

# 3. Opcional: Borra modelos
rm data/models/*.pt
```

---

## ğŸ“š DocumentaciÃ³n Adicional

- **SKILLS/learning/SKILL.md** - DocumentaciÃ³n tÃ©cnica completa
- **SKILLS/learning/patterns/actor-critic.md** - Algoritmos Actor-Critic
- **SKILLS/learning/assets/README.md** - GuÃ­a de scripts
- **SUBAGENTS/universal/actor-critic-learner.md** - Agente de aprendizaje

---

## ğŸ¯ Checklist de ImplementaciÃ³n

```yaml
[ ] Instalar ai-core en proyecto
[ ] Habilitar modo shadow (AI_CORE_LEARNING_MODE=shadow)
[ ] Recolectar 1000+ experiencias (2-4 semanas)
[ ] Entener primer modelo
[ ] Evaluar modelo (accuracy > 70%, reward > 0)
[ ] Comparar con baseline (mejora > 10%)
[ ] Habilitar ab_test (AI_CORE_LEARNING_MODE=ab_test)
[ ] Monitorear por 1 semana
[ ] Si mejora > 15%, habilitar production
[ ] Mantener monitoreo continuo
```

---

**Â¿Listo para empezar?** El sistema de aprendizaje es opcional. AI-Core funciona perfectamente sin Ã©l. Solo Ãºsalo si quieres optimizaciÃ³n automÃ¡tica basada en datos.

**Â¿Necesitas ayuda?** Ver la documentaciÃ³n tÃ©cnica en `SKILLS/learning/SKILL.md` o crea un issue en GitHub.
