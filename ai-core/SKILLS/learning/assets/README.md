# Actor-Critic Training Scripts

MÃ­nimo viable para validar que el sistema Actor-Critic funciona en ai-core.

## ğŸ§ª QUICK START (Test with Synthetic Data)

Want to test the pipeline immediately without waiting for real data?

```bash
# Run complete end-to-end test
./SKILLS/learning/assets/test_pipeline.sh
```

This will:
1. âœ… Generate 1000 synthetic test experiences
2. âœ… Train Actor-Critic model (50 epochs)
3. âœ… Evaluate model performance
4. âœ… Compare with rule-based baseline
5. âœ… Generate monitoring reports

**Expected results:**
- Model learns from synthetic data
- Mean reward becomes positive
- Success rate > 70%
- Some improvement over baseline

**Manual testing:**

```bash
# 1. Generate test data
python SKILLS/learning/assets/generate_test_data.py \
  --count 1000 \
  --output data/experience_buffer/test_experiences.jsonl

# 2. Train model
python SKILLS/learning/assets/train.py \
  --data data/experience_buffer/test_experiences.jsonl \
  --epochs 50

# 3. Evaluate
python SKILLS/learning/assets/evaluate.py \
  --model data/models/actor_checkpoint_v1.0.pt \
  --test-data data/experience_buffer/test_experiences.jsonl

# 4. Monitor
python SKILLS/learning/assets/monitor.py \
  --data data/experience_buffer/test_experiences.jsonl
```

---

## ğŸ“ Scripts Disponibles

### 1. `models.py`
ImplementaciÃ³n de redes neuronales PyTorch:
- `ActorNetwork`: Red de polÃ­tica (state â†’ action probabilities)
- `CriticNetwork`: Red de valor (state â†’ V(s))
- `ActorCriticAgent`: Agente combinado
- `train_a2c()`: Algoritmo de entrenamiento A2C
- `save_checkpoint()` / `load_checkpoint()`: Guardar/cargar modelos

### 2. `train.py`
Entrena modelo Actor-Critic sobre experiencias recolectadas.

```bash
python train.py --data data/experience_buffer/experiences.jsonl
```

**Opciones:**
- `--data`: Ruta a experiencias (default: `data/experience_buffer/experiences.jsonl`)
- `--epochs`: NÃºmero de epochs (default: 100)
- `--batch-size`: TamaÃ±o de batch (default: 64)
- `--learning-rate`: Learning rate (default: 1e-4)
- `--state-dim`: DimensiÃ³n del estado (default: 50)
- `--action-dim`: DimensiÃ³n de la acciÃ³n (default: 30)
- `--output-dir`: Directorio de salida (default: `data/models`)

**Salida:**
- `data/models/actor_checkpoint_v1.0.pt`: Modelo final
- `data/models/actor_checkpoint_best.pt`: Mejor modelo (validaciÃ³n)
- `data/models/training_history.json`: Historial de entrenamiento

### 3. `evaluate.py`
EvalÃºa modelo entrenado en conjunto de prueba.

```bash
python evaluate.py --model data/models/actor_checkpoint_v1.0.pt
```

**Opciones:**
- `--model`: Ruta al modelo entrenado (requerido)
- `--test-data`: Ruta a datos de prueba
- `--output`: Ruta para guardar reporte (JSON)

**MÃ©tricas:**
- Accuracy de predicciÃ³n
- Recompensa media
- Success rate
- CorrelaciÃ³n con valores reales
- AnÃ¡lisis por tipo de tarea

### 4. `monitor.py`
Monitorea experiencias recolectadas en tiempo real.

```bash
python monitor.py --data data/experience_buffer/experiences.jsonl
```

**Opciones:**
- `--data`: Ruta a experiencias
- `--output`: Guardar reporte a JSON
- `--watch`: Modo watch (refresca cada 30s)
- `--interval`: Intervalo de refresco (segundos)

**Dashboard muestra:**
- EstadÃ­sticas generales
- AnÃ¡lisis por tipo de tarea
- AnÃ¡lisis por dominio
- AnÃ¡lisis por complejidad
- Health check
- Recomendaciones

### 5. `compare_policies.py`
Compara polÃ­tica aprendida con baseline basado en reglas.

```bash
python compare_policies.py --learned data/models/actor_checkpoint_v1.0.pt
```

**Opciones:**
- `--learned`: Ruta a modelo aprendido (requerido)
- `--test-data`: Ruta a datos de prueba
- `--output`: Guardar reporte a JSON

**Compara:**
- Recompensas medias
- Success rates
- Patrones de decisiÃ³n
- Porcentaje de acuerdo

### 6. `generate_test_data.py`
Genera datos de prueba sintÃ©ticos realistas.

```bash
python generate_test_data.py --count 1000 --output data/experience_buffer/test.jsonl
```

**Opciones:**
- `--count`: NÃºmero de experiencias (default: 1000)
- `--output`: Archivo de salida
- `--seed`: Semilla aleatoria para reproducibilidad

**Genera:**
- Experiencias con todos los tipos de tareas
- DistribuciÃ³n realista de dominios
- Complejidades variadas
- Recompensas realistas

### 7. `test_pipeline.sh`
Script completo de prueba end-to-end.

```bash
./test_pipeline.sh
```

**Ejecuta todos los pasos:**
1. Genera datos de prueba
2. Entrena modelo
3. EvalÃºa modelo
4. Compara con baseline
5. Genera reportes

---

## ğŸš€ Flujo de Trabajo Completo

### Paso 1: Recolectar Experiencias

```bash
# Habilitar modo shadow
export AI_CORE_LEARNING_MODE=shadow

# Usar ai-core normalmente
# Las experiencias se recolectan automÃ¡ticamente

# Verificar recolecciÃ³n
python -m learning.assets.monitor --data data/experience_buffer/experiences.jsonl
```

**MÃ­nimo recomendado:** 100 experiencias
**Ideal:** 1,000+ experiencias

### Paso 2: Entrenar Modelo

```bash
# Entrenar con opciones por defecto
python SKILLS/learning/assets/train.py \
  --data data/experience_buffer/experiences.jsonl \
  --epochs 100 \
  --output-dir data/models
```

**QuÃ© esperar:**
- Epoch 0-30: Actor/Critic loss disminuyen
- Epoch 30-70: Recompensa mejora gradualmente
- Epoch 70-100: Convergencia

**Indicadores de Ã©xito:**
- âœ… Mean reward > 0
- âœ… Actor loss disminuye
- âœ… Critic loss converge
- âœ… Entropy > 0.1

### Paso 3: Evaluar Modelo

```bash
# Evaluar en conjunto de prueba
python SKILLS/learning/assets/evaluate.py \
  --model data/models/actor_checkpoint_v1.0.pt \
  --test-data data/experience_buffer/experiences.jsonl \
  --output data/metrics/evaluation_report.json
```

**Criterios de Ã©xito:**
- âœ… Accuracy > 70%
- âœ… Success rate > 85%
- âœ… Mean reward > 0
- âœ… CorrelaciÃ³n > 0.3

### Paso 4: Comparar con Baseline

```bash
# Comparar con reglas
python SKILLS/learning/assets/compare_policies.py \
  --learned data/models/actor_checkpoint_v1.0.pt \
  --output data/metrics/comparison_report.json
```

**QuÃ© buscar:**
- âœ… Improvement > 5%
- âš ï¸  Improvement > 0% (marginal)
- âŒ Improvement < 0% (empeorar)

### Paso 5: Monitoreo Continuo

```bash
# Monitoreo one-time
python SKILLS/learning/assets/monitor.py --data data/experience_buffer/experiences.jsonl

# Monitoreo continuo
python SKILLS/learning/assets/monitor.py \
  --data data/experience_buffer/experiences.jsonl \
  --watch \
  --interval 30
```

---

## ğŸ“Š InterpretaciÃ³n de Resultados

### Entrenamiento Exitoso

```
Epoch  90: Actor Loss=0.0234, Critic Loss=0.0156, Train Reward=45.30, Val Reward=42.10, Entropy=0.3456

âœ… FINAL METRICS:
  Actor Loss: 0.0234 (bajo)
  Critic Loss: 0.0156 (convergido)
  Mean Reward: 45.30 (positivo âœ…)
  Entropy: 0.3456 (exploraciÃ³n suficiente âœ…)

âœ… SUCCESS: Model learned positive reward policy
```

### Entrenamiento ProblemÃ¡tico

```
Epoch  90: Actor Loss=0.5234, Critic Loss=0.8156, Train Reward=-35.30, Val Reward=-38.10, Entropy=0.0156

âŒ FINAL METRICS:
  Actor Loss: 0.5234 (alto âŒ)
  Critic Loss: 0.8156 (no converge âŒ)
  Mean Reward: -35.30 (negativo âŒ)
  Entropy: 0.0156 (sin exploraciÃ³n âŒ)

âŒ FAILED: Model did not learn effectively
```

---

## ğŸ› ï¸ Troubleshooting

### Error: "No experiences found"

```bash
# Verificar que hay experiencias
wc -l data/experience_buffer/experiences.jsonl

# Si estÃ¡ vacÃ­o, ejecutar en shadow mode
export AI_CORE_LEARNING_MODE=shadow
# Usar ai-core para generar experiencias
```

### Error: "Model did not learn"

**Causas posibles:**
1. Pocas experiencias (< 100)
   - SoluciÃ³n: Recolectar mÃ¡s datos

2. FunciÃ³n de reward mal diseÃ±ada
   - SoluciÃ³n: Revisar `compute_reward()` en `train.py`

3. HiperparÃ¡metros inadecuados
   - SoluciÃ³n: Ajustar `--learning-rate` o `--epochs`

4. Datos de baja calidad
   - SoluciÃ³n: Revisar experiencias recolectadas

### Warning: "Low experience count"

```bash
# Continuar recolectando mÃ¡s datos
export AI_CORE_LEARNING_MODE=shadow
# Usar ai-core normalmente
```

---

## ğŸ“ˆ PrÃ³ximos Pasos

### Si el modelo funciona bien:

1. **A/B Testing**
   ```bash
   export AI_CORE_LEARNING_MODE=ab_test
   # El sistema usarÃ¡ learned policy 10% del tiempo
   ```

2. **ProducciÃ³n Gradual**
   ```bash
   export AI_CORE_LEARNING_MODE=production
   # Usar learned policy con fallback a reglas
   ```

3. **Continuous Learning**
   - Reentrenar periÃ³dicamente con nuevas experiencias
   - Monitorear performance en producciÃ³n
   - A/B test contra nuevas versiones

### Si el modelo necesita mejora:

1. **MÃ¡s Datos**
   - Recolectar 1000+ experiencias
   - Asegurar cobertura de todos los tipos de tareas

2. **IngenierÃ­a de Features**
   - Mejorar representaciÃ³n del estado
   - Agregar mÃ¡s contexto histÃ³rico

3. **HiperparÃ¡metros**
   - Probar diferentes learning rates
   - Ajustar arquitectura de la red
   - Cambiar batch size

4. **Reward Function**
   - Revisar pesos de componentes
   - Agregar nuevas seÃ±ales de reward
   - Balancear objetivos mÃºltiples

---

## ğŸ“š Referencias

- `../SKILL.md`: DocumentaciÃ³n principal del skill learning
- `../patterns/actor-critic.md`: Detalles del algoritmo
- `../patterns/reinforcement-learning.md`: Fundamentos de RL
- `../../../SUBAGENTS/universal/actor-critic-learner.md`: Agente de aprendizaje

---

**EOF**
