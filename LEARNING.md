# Orchestra Learning System

**Reinforcement Learning para OptimizaciÃ³n de OrquestaciÃ³n**

El Learning System de Orchestra utiliza aprendizaje por refuerzo (Actor-Critic) para mejorar continuamente la selecciÃ³n de recursos y estrategias de ejecuciÃ³n basÃ¡ndose en experiencias pasadas.

---

## ğŸ¯ Objetivo

Optimizar automÃ¡ticamente la orquestaciÃ³n aprendiendo de cada ejecuciÃ³n:
- SelecciÃ³n Ã³ptima de agentes (Architect, Executor, Auditor, Consultant)
- Estrategia de ejecuciÃ³n ideal (secuencial, paralela, pipeline)
- PredicciÃ³n de tiempo y recursos necesarios
- ReducciÃ³n de errores y mejora de calidad

---

## ğŸ—ï¸ Arquitectura

### Ciclo DiseÃ±o-ConstrucciÃ³n-Prueba-Aprendizaje

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DiseÃ±o    â”‚ â†’ Architect: Crea plan de implementaciÃ³n
â”‚  (Architect) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ConstrucciÃ³nâ”‚ â†’ Executor: Genera cÃ³digo
â”‚  (Executor)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prueba    â”‚ â†’ Auditor: Revisa cÃ³digo + Tests automÃ¡ticos
â”‚  (Auditor)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aprendizaje â”‚ â†’ Experience Collection + Reward Calculation
â”‚  (Learning)  â”‚ â†’ Policy Update (Actor-Critic)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
    [Mejora continua]
```

### Componentes Principales

1. **ExperienceCollector** (`src/learning/ExperienceCollector.ts`)
   - Captura experiencias (state, action, reward) de cada ejecuciÃ³n
   - Almacena en buffer persistente (JSONL)
   - Normaliza features y computa rewards

2. **LearningManager** (`src/learning/LearningManager.ts`)
   - Gestiona modos de operaciÃ³n
   - Carga y despliega polÃ­ticas aprendidas
   - Coordina colecciÃ³n y entrenamiento

3. **OrchestratorIntegration** (`src/learning/OrchestratorIntegration.ts`)
   - Integra Learning System con Orchestrator
   - Extrae contexto y mÃ©tricas de ejecuciÃ³n
   - Infiere task type, domain, complexity, risk level

4. **CLI Commands** (`src/cli/learningCommands.ts`)
   - Comandos para controlar el sistema de aprendizaje
   - VisualizaciÃ³n de estadÃ­sticas
   - ExportaciÃ³n de experiencias

---

## ğŸ“Š State Representation

### State Vector (~50-100 dims)

```yaml
Task Features:
  - task_type: one-hot(13)     # feature, bug, refactor, test, docs, etc.
  - domain: one-hot(9)          # frontend, backend, database, devops, etc.
  - complexity: ordinal(3)      # simple, medium, complex
  - risk_level: ordinal(3)      # low, medium, high

Context Features (normalized 0-1):
  - estimated_time: float       # Tiempo estimado normalizado
  - domain_diversity: float     # NÃºmero de dominios involucrados
  - skill_count: float          # NÃºmero de skills necesarios

Historical Features:
  - success_rate: float         # Tasa de Ã©xito en tareas similares
  - time_accuracy: float        # PrecisiÃ³n de estimaciones de tiempo
  - resource_efficiency: float  # Eficiencia en uso de recursos

System State:
  - concurrent_tasks: float     # Tareas concurrentes
  - system_load: float          # Carga del sistema
  - agent_availability: dict    # Disponibilidad de agentes
```

### Action Vector

```yaml
Resources:
  - skills: list[str]           # Skills a usar
  - agents: list[str]           # Agentes a invocar

Strategy:
  - approach: enum              # direct, sequential, parallel, coordinated

Parameters:
  - timeout_multiplier: float   # [0.5, 2.0]
  - parallelism: int            # [1, 4]
  - retry_strategy: enum        # fail_fast, retry_with_backoff, fallback
  - safety_level: enum          # strict, balanced, permissive
```

---

## ğŸ’° Reward Function (Enhanced with Cost Optimization)

Rango: [-100, +180]

```python
reward = (
    success * 100 +                          # Ã‰xito/fallo (dominante)
    time_efficiency * 20 +                   # Velocidad de ejecuciÃ³n
    resource_efficiency * 10 +               # Eficiencia de recursos
    (zero_errors ? 15 : -errors*10) +        # Calidad (errores)
    user_satisfaction * 10 +                 # Modificaciones del usuario
    safety_adherence * 10 +                  # Cumplimiento de seguridad
    (tests_passed ? 5 : 0) +                 # Tests exitosos
    cost_efficiency_bonus +                  # NUEVO: BonificaciÃ³n por bajo costo
    glm_usage_bonus +                        # NUEVO: BonificaciÃ³n por uso de GLM
    expensive_model_penalty +                # NUEVO: PenalizaciÃ³n por modelos caros
    fallback_penalty                         # NUEVO: PenalizaciÃ³n por fallbacks
)
```

### Componentes del Reward

| Componente | Peso | DescripciÃ³n |
|------------|------|-------------|
| Success | Â±100 | Dominante: tarea completada vs fallida |
| Time Efficiency | +20 | Ratio estimado/real (<2x = mejor) |
| Resource Efficiency | Â±10 | Uso mÃ­nimo de recursos |
| Quality | Â±15 | Cero errores vs mÃºltiples errores |
| User Satisfaction | Â±10 | Sin modificaciones post-generaciÃ³n |
| Safety | +10/-50 | Cumplimiento vs violaciÃ³n de seguridad |
| Tests | +5 | Tests pasando exitosamente |
| **Cost Efficiency** | **+50** | **ğŸ†• SesiÃ³n muy econÃ³mica (< $0.10)** |
| **GLM Usage** | **+10/success** | **ğŸ†• BonificaciÃ³n por cada Ã©xito con GLM-4.7** |
| **Expensive Models** | **-5/usage** | **ğŸ†• PenalizaciÃ³n por uso excesivo de modelos caros** |
| **Fallback Rotations** | **-10/rotation** | **ğŸ†• PenalizaciÃ³n por fallbacks** |

### Nuevo: Tracking de Modelos

El sistema ahora rastrea el rendimiento detallado de cada modelo:

```typescript
interface ModelUsage {
  modelId: string;                    // "glm-4.7", "kimi-k2.5", etc.
  provider: string;                   // "zai", "moonshot", "openai", etc.
  tokensUsed: number;                 // Tokens consumidos
  latencyMs: number;                  // Latencia en ms
  success: boolean;                   // Â¿Exitoso?
  errorCode?: 'RATE_LIMIT_429' | 'CONTEXT_EXCEEDED' | 'TIMEOUT' | 'API_ERROR';
  errorMessage?: string;              // Mensaje de error
  timestamp: string;                  // ISO timestamp
  estimatedCost?: number;             // Costo estimado en USD
}
```

### CÃ¡lculo de Recompensas Optimizado

```typescript
private calculateReward(state: EnhancedSessionState): number {
  let reward = 0;

  // Base: Ã©xito/fallo
  reward += state.phase === 'completed' ? 100 : -100;

  // ğŸ†• Eficiencia de costo
  if (state.globalMetrics.totalCostEstimate < 0.10) {
    reward += 50; // Muy econÃ³mico
  } else if (state.globalMetrics.totalCostEstimate > 0.50) {
    reward -= 20; // Muy costoso
  }

  // ğŸ†• BonificaciÃ³n por uso de GLM (econÃ³mico)
  const glmSuccesses = state.workflow
    .flatMap(s => s.attempts)
    .filter(a => a.modelId.includes('glm') && a.success)
    .length;
  reward += glmSuccesses * 10;

  // ğŸ†• PenalizaciÃ³n por uso excesivo de modelos caros (GPT-5.2-Codex)
  const expensiveUsages = state.workflow
    .flatMap(s => s.attempts)
    .filter(a => a.modelId.includes('gpt-5.2') || a.modelId.includes('opus'))
    .length;
  if (expensiveUsages > 3) {
    reward -= (expensiveUsages - 3) * 5; // -5 por cada uso extra
  }

  // ğŸ†• PenalizaciÃ³n por fallback rotations
  reward -= state.globalMetrics.fallbackRotations * 10;

  // Eficiencia de tiempo
  const timeEfficiency = state.globalMetrics.avgLatencyMs < 30000 ? 1.0 : 0.5;
  reward += timeEfficiency * 20;

  // Calidad (sin errores)
  const errorCount = state.workflow.filter(s => s.status === 'failed').length;
  reward += errorCount === 0 ? 15 : -errorCount * 10;

  return reward;
}
```

### ğŸ†• Global Metrics Tracking

El sistema ahora mantiene mÃ©tricas globales de cada sesiÃ³n para optimizaciÃ³n de costos:

```typescript
interface GlobalMetrics {
  totalCostEstimate: number;          // Costo total estimado en USD
  startTime: number;                  // Timestamp de inicio
  endTime?: number;                   // Timestamp de fin
  totalTokens: number;                // Total de tokens consumidos
  totalAttempts: number;              // Total de intentos realizados
  successfulAttempts: number;         // Intentos exitosos
  failedAttempts: number;             // Intentos fallidos
  fallbackRotations: number;          // NÃºmero de rotaciones de fallback
  avgLatencyMs: number;               // Latencia promedio en ms
}
```

**Beneficios:**
- **Visibilidad de costos**: Tracking en tiempo real del costo de cada sesiÃ³n
- **OptimizaciÃ³n automÃ¡tica**: El sistema aprende a minimizar costos usando modelos econÃ³micos
- **DetecciÃ³n de problemas**: Identifica patrones de fallbacks excesivos o errores repetidos
- **AnÃ¡lisis de rendimiento**: Compara latencia y eficiencia entre modelos

### ğŸ†• Workflow Step Tracking

Cada paso del workflow se rastrea con detalle completo:

```typescript
interface TaskStep {
  id: string;                         // Identificador Ãºnico
  agentRole: 'architect' | 'executor' | 'auditor' | 'consultant';
  status: 'pending' | 'running' | 'completed' | 'failed';
  filePath?: string;                  // Archivo afectado
  attempts: ModelUsage[];             // Historial de intentos
  outputHash?: string;                // Hash del output (para deduplicaciÃ³n)
  startTime?: string;                 // ISO timestamp de inicio
  endTime?: string;                   // ISO timestamp de fin
  duration?: number;                  // DuraciÃ³n en ms
}
```

**Ejemplo de workflow completo:**

```json
{
  "workflow": [
    {
      "id": "step-1",
      "agentRole": "architect",
      "status": "completed",
      "attempts": [
        {
          "modelId": "kimi-k2.5",
          "provider": "moonshot",
          "tokensUsed": 1500,
          "latencyMs": 4500,
          "success": true,
          "estimatedCost": 0.045
        }
      ],
      "duration": 4500
    },
    {
      "id": "step-2",
      "agentRole": "executor",
      "status": "completed",
      "filePath": "src/auth/login.ts",
      "attempts": [
        {
          "modelId": "glm-4.7",
          "provider": "zai",
          "tokensUsed": 2000,
          "latencyMs": 3000,
          "success": true,
          "estimatedCost": 0.010
        }
      ],
      "duration": 3000
    }
  ],
  "globalMetrics": {
    "totalCostEstimate": 0.055,
    "totalTokens": 3500,
    "totalAttempts": 2,
    "successfulAttempts": 2,
    "failedAttempts": 0,
    "fallbackRotations": 0,
    "avgLatencyMs": 3750
  }
}
```

---

## ğŸ”§ Modos de OperaciÃ³n

### 1. Disabled (Default)
```bash
export ORCHESTRA_LEARNING_MODE=disabled
```
- **Comportamiento**: Sin aprendizaje, solo reglas
- **Uso**: Cuando no quieres usar learning
- **ColecciÃ³n**: No colecta experiencias

### 2. Shadow Mode
```bash
export ORCHESTRA_LEARNING_MODE=shadow
```
- **Comportamiento**: Colecta experiencias, usa reglas
- **Uso**: Fase inicial para construir dataset
- **ColecciÃ³n**: âœ… Activa
- **PolÃ­tica**: No usa aprendida, solo reglas

**Recomendado para comenzar:**
```bash
# 1. Activar shadow mode
export ORCHESTRA_LEARNING_MODE=shadow

# 2. Ejecutar tareas normalmente
orchestra start "Add authentication"
orchestra start "Fix database query bug"
orchestra start "Refactor user service"

# 3. Ver estadÃ­sticas
orchestra learning-stats

# 4. Exportar para entrenamiento (cuando tengas suficientes)
orchestra learning-export -o experiences.json
```

### 3. A/B Test Mode
```bash
export ORCHESTRA_LEARNING_MODE=ab_test
```
- **Comportamiento**: 10% polÃ­tica aprendida, 90% reglas
- **Uso**: Testing de polÃ­tica entrenada
- **ColecciÃ³n**: âœ… Activa
- **PolÃ­tica**: Usa aprendida probabilÃ­sticamente

### 4. Production Mode
```bash
export ORCHESTRA_LEARNING_MODE=production
```
- **Comportamiento**: 100% polÃ­tica aprendida con fallback
- **Uso**: Deployment final
- **ColecciÃ³n**: âœ… Activa
- **PolÃ­tica**: Usa aprendida con fallback a reglas

---

## ğŸ“ Estructura de Archivos

```
src/learning/
â”œâ”€â”€ types.ts                      # Tipos TypeScript
â”œâ”€â”€ ExperienceCollector.ts        # ColecciÃ³n de experiencias
â”œâ”€â”€ LearningManager.ts            # Manager principal
â”œâ”€â”€ OrchestratorIntegration.ts    # IntegraciÃ³n con Orchestrator
â””â”€â”€ index.ts                      # Exports

src/cli/
â””â”€â”€ learningCommands.ts           # Comandos CLI

data/
â”œâ”€â”€ experience_buffer/
â”‚   â””â”€â”€ experiences.jsonl         # Buffer persistente (JSONL)
â”œâ”€â”€ models/                       # Modelos entrenados (futuro)
â””â”€â”€ metrics/                      # MÃ©tricas de entrenamiento
```

---

## ğŸš€ GuÃ­a de Uso

### Paso 1: Activar Shadow Mode

```bash
# Configurar modo
export ORCHESTRA_LEARNING_MODE=shadow

# O agregar a tu shell profile
echo 'export ORCHESTRA_LEARNING_MODE=shadow' >> ~/.bashrc
source ~/.bashrc
```

### Paso 2: Ejecutar Tareas

```bash
# Ejecutar tareas normalmente
orchestra start "Add user authentication API"
orchestra start "Refactor database queries for performance"
orchestra start "Fix memory leak in backend service"
orchestra start "Add unit tests for user controller"
```

### Paso 3: Monitorear EstadÃ­sticas

```bash
# Ver estado del learning system
orchestra learning

# Ver estadÃ­sticas detalladas
orchestra learning-stats

# Salida ejemplo:
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#             LEARNING SYSTEM STATISTICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# Mode: SHADOW
# Policy Loaded: No
#
# Experience Buffer:
#   Total experiences: 127
#   Mean reward: 85.3
#   Success rate: 92.1%
#
# By Task Type:
#   feature         45
#   bug             32
#   refactor        28
#   test            22
#
# By Domain:
#   backend         67
#   frontend        34
#   database        26
```

### Paso 4: Exportar Experiencias

```bash
# Exportar para entrenamiento
orchestra learning-export -o training-data.json

# Salida:
# â†’ Exporting experiences...
# âœ“ Experiences exported to: training-data.json
#
# Statistics:
#   Total experiences: 127
#   Mean reward: 85.3
#   Success rate: 92.1%
```

### Paso 5: Entrenar PolÃ­tica (Futuro)

```bash
# Entrenar polÃ­tica con Actor-Critic
orchestra learning-train --data training-data.json --epochs 100 --output models/

# âš  Training not yet implemented
# The Actor-Critic policy training will be available in a future update.
```

### Paso 6: Desplegar (Futuro)

```bash
# Cambiar a A/B test mode
orchestra learning-mode ab_test

# Si va bien, cambiar a production
orchestra learning-mode production
```

---

## ğŸ“ˆ MÃ©tricas y Monitoring

### MÃ©tricas de Experience Buffer

```bash
orchestra learning-stats
```

- **Total experiences**: NÃºmero de ejecuciones capturadas
- **Mean reward**: Reward promedio (objetivo: >80)
- **Success rate**: Tasa de Ã©xito (objetivo: >85%)
- **By task type**: DistribuciÃ³n por tipo de tarea
- **By domain**: DistribuciÃ³n por dominio

### MÃ©tricas de Policy (Futuro)

- **Accuracy**: PrecisiÃ³n de predicciones
- **Resource efficiency**: Eficiencia vs baseline
- **Time accuracy**: PrecisiÃ³n de estimaciones
- **Safety compliance**: Cumplimiento de seguridad

---

## ğŸ“ Mejores PrÃ¡cticas

### 1. ColecciÃ³n de Datos

âœ… **DO:**
- Ejecuta tareas diversas (features, bugs, refactors, tests)
- Cubre mÃºltiples dominios (frontend, backend, database)
- Colecta al menos 100-200 experiencias antes de entrenar
- MantÃ©n balanceo entre tareas simples y complejas

âŒ **DON'T:**
- No entrenes con menos de 50 experiencias
- No colectes solo un tipo de tarea
- No ignores tareas fallidas (son valiosas)

### 2. Entrenamiento

âœ… **DO:**
- Empieza con shadow mode
- Valida en test set antes de deployment
- Compara contra baseline de reglas
- Monitorea mÃ©tricas continuamente

âŒ **DON'T:**
- No pases directamente a production sin A/B testing
- No ignores safety violations
- No desactives fallback a reglas

### 3. Deployment

âœ… **DO:**
- Usa A/B test mode primero (10% learned)
- Monitorea por 24-48 horas
- Incrementa gradualmente (25% â†’ 50% â†’ 100%)
- MantÃ©n plan de rollback

âŒ **DON'T:**
- No despliegues sin testing
- No remuevas fallback mechanisms
- No ignores anomalÃ­as

---

## ğŸ”® Futuro: Actor-Critic Networks

### Arquitectura Planificada

```
Estado (state_dim) â†’ [Actor Network] â†’ AcciÃ³n (action_dim)
                        â†“
                   [Critic Network] â†’ Value (V(s))
                        â†“
                   [Training Loop] â†’ Policy Update
```

### Actor Network
- **Input**: State vector (~50-100 dims)
- **Hidden**: [256, 128] fully connected
- **Output**: Action probabilities (softmax)

### Critic Network
- **Input**: State vector (~50-100 dims)
- **Hidden**: [256, 128] fully connected
- **Output**: State value V(s) (scalar)

### Training Algorithm
- **A2C** (Advantage Actor-Critic)
- **Batch size**: 64-128
- **Learning rate**: 0.0001-0.001
- **Optimizer**: Adam
- **Loss**: Actor loss + Critic loss

---

## ğŸ› Troubleshooting

### No se colectan experiencias

**SÃ­ntoma**: `orchestra learning-stats` muestra 0 experiencias

**SoluciÃ³n**:
```bash
# Verificar modo
orchestra learning-mode

# Activar shadow mode
export ORCHESTRA_LEARNING_MODE=shadow
orchestra learning-mode shadow
```

### Reward muy bajo

**SÃ­ntoma**: Mean reward < 0

**Causa posible**: Muchos fallos o errores

**SoluciÃ³n**:
- Revisa logs de ejecuciÃ³n
- Ajusta configuraciÃ³n de timeouts
- Mejora prompts de agentes

### Policy no carga

**SÃ­ntoma**: "Policy Loaded: No"

**Causa**: No hay modelo entrenado aÃºn

**SoluciÃ³n**: Esto es normal. Training viene en futuras versiones.

---

## ğŸ“š Referencias

- [ai-core Learning System](./ai-core/SKILLS/learning/SKILL.md)
- [ai-core Actor-Critic Learner](./ai-core/SUBAGENTS/universal/actor-critic-learner.md)
- [ADR 005: Learning System](./ai-core/docs/adr/005-learning-system.md)
- [Reinforcement Learning Patterns](./ai-core/SKILLS/learning/patterns/reinforcement-learning.md)

---

## ğŸ¤ Contributing

El Learning System estÃ¡ en desarrollo activo. Contribuciones bienvenidas:

1. **Experience Collection**: Mejorar inferencia de features
2. **Reward Function**: Ajustar pesos y componentes
3. **Actor-Critic**: Implementar training con TensorFlow.js
4. **UI/UX**: Mejorar visualizaciÃ³n de mÃ©tricas

---

**Estado**: âœ… Experience Collection implementado, ğŸ”„ Training en desarrollo

**VersiÃ³n**: 1.0.0

**Ãšltima actualizaciÃ³n**: 2026-02-05
